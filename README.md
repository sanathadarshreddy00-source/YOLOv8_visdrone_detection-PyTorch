# YOLOv8(PyTorch) â€” VisDrone Object Detection

One-line: Reproducible YOLOv8 detection pipeline for the VisDrone 2019 dataset â€” training, evaluation, and demo video generation.

---

## Quickstart

- Create and activate environment (example):

```powershell
conda create -n cv310 python=3.10 -y
conda activate cv310
```

- Install dependencies (if `requirements.txt` present):

```powershell
pip install -r requirements.txt
# or at minimum:
pip install ultralytics opencv-python pyyaml
```

- If Python cannot import `src`, set `PYTHONPATH` at project root before running scripts:

```powershell
$env:PYTHONPATH = "$(Get-Location)"
```

Run the three main steps (copy/paste):

```powershell
# Train (example)
=======
# VisDrone Object Detection with YOLOv8

Real-time object detection pipeline for aerial imagery using YOLOv8 on the VisDrone 2019 dataset.

**Project Status:** âœ… Complete  
**Achieved:** **36.2% mAP@0.5**
**Model:** YOLOv8s (11.1M parameters)

---

## ðŸŽ¯ Project Summary

Successfully trained and deployed YOLOv8s for detecting 10 object classes in dense aerial imagery. The project demonstrates end-to-end pipeline development from data conversion to video inference, with emphasis on handling extreme small-object detection challenges under hardware constraints.

---

## ðŸ“ Project Structure (selected files)

```
Second Project/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ visdrone.yaml                # dataset paths & class mapping
â”‚   â”œâ”€â”€ experiment_config.yaml       # training/experiment hyperparams
â”‚   â””â”€â”€ paths.yaml                   # canonicalized project paths used by scripts
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                        # dataset loaders and transforms
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ paths.py                 # loads `configs/paths.yaml` (central path resolver)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_convert_annotations.py
â”‚   â”œâ”€â”€ 02_verify_data.py
â”‚   â”œâ”€â”€ 03_prepare_dataset.py
â”‚   â”œâ”€â”€ 04_train.py
â”‚   â”œâ”€â”€ 05_evaluate.py
â”‚   â””â”€â”€ 06_video_inference.py
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ generate_directory_record.py # creates DIRECTORY_RECORD_FULL.{csv,json} (gitignored)
â”‚   â”œâ”€â”€ sanitize_directory_record.py # emits DIRECTORY_RECORD_SUMMARY.json/.csv (commit-friendly)
â”‚   â””â”€â”€ purge_zero_byte_files.py     # safe purge helpers using inventory
â”œâ”€â”€ runs/detect/
â”‚   â”œâ”€â”€ yolov8s_20260224_1745502/    # canonical run (weights, args.yaml, predictions, demos)
â”‚   â””â”€â”€ videos/                       # compiled demo MP4s (e.g. demo_YYYYMMDD_HHMMSS/)
â”œâ”€â”€ DEVELOPER_GUIDE.md
â”œâ”€â”€ MIGRATION_MANIFEST_20260224_154353.csv  # migration manifest of moved legacy runs
â”œâ”€â”€ DIRECTORY_RECORD_SUMMARY.json    # sanitized inventory snapshot (small, safe to commit)
â”œâ”€â”€ DIRECTORY_RECORD_FULL.csv/.json  # full inventory (large â€” gitignored; generated by `tools/`)
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ðŸš€ Quick Start

### 1. Install Dependencies

```bash
# Activate your conda environment
conda activate cv310

# Install requirements
pip install -r requirements.txt
```

### 2. Run Pipeline

Execute scripts in order:

```bash
# Step 1: Convert annotations (VisDrone â†’ YOLO format)
python scripts/01_convert_annotations.py

# Step 2: Verify conversion visually
python scripts/02_verify_data.py

# Step 3: Prepare dataset (train/val split + file organization)
python scripts/03_prepare_dataset.py

# Step 4: Train YOLOv8 model
>>>>>>> origin/main
python scripts/04_train.py

# Evaluate canonical model (conf=0.001)
python scripts/05_evaluate.py --weights runs/detect/yolov8s_20260224_1745502/weights/best.pt --conf 0.001

# Generate demo video for a sequence
python scripts/06_video_inference.py --video VisDrone2019-VID-val/sequences/uav0000137_00458_v --weights runs/detect/yolov8s_20260224_1745502/weights/best.pt --conf 0.3 --save
```

---

- Experiment name: `yolov8s_20260224_1745502`
- Weights: `runs/detect/yolov8s_20260224_1745502/weights/best.pt`
- Training args (excerpt): `imgsz=640`, `batch=4`, `workers=0`, `seed=42`
- Eval command used for reported metrics:

```bash
python scripts/05_evaluate.py --weights runs/detect/yolov8s_20260224_1745502/weights/best.pt --conf 0.001
```

Canonical metrics (validation, conf=0.001):

- mAP@0.5: **36.35%**
- mAP@0.5:0.95: **21.21%**
- Precision: **47.10%**
- Recall: **36.35%**

Per-class mAP@0.5 (conf=0.001):

| Class | mAP@0.5 |
|---|---:|
| pedestrian | 21.76% |
| people | 13.75% |
| bicycle | 10.69% |
| car | 73.78% |
| van | 42.95% |
| truck | 56.66% |
| tricycle | 20.58% |
| awning-tricycle | 29.25% |
| bus | 66.97% |
| motor | 27.14% |

Demo examples (produced under `runs/detect/videos/`):

- Example demo folder: `runs/detect/videos/demo_20260224_224656/`
- Example MP4: `runs/detect/videos/demo_20260224_224656/uav0000137_00458_v.mp4` (233 frames processed)

---

## Project layout (key folders)

- `configs/` â€” dataset and experiment YAMLs (`visdrone.yaml`, `experiment_config.yaml`, `paths.yaml`).
- `src/` â€” reusable utilities (paths loader, reproducibility, visualization).
- `scripts/` â€” runnable scripts: conversion, verification, prepare dataset, train, evaluate, video inference.
- `tools/` â€” helper tools (inventory generation, maintenance).
- `runs/detect/` â€” training experiment folders and demo outputs (weights and demo videos).
- `VisDrone2019-VID-val/` â€” video sequences used for demo inference.

---

## Script usage (short)

- `scripts/01_convert_annotations.py`: Convert VisDrone annotations to YOLO format.
- `scripts/02_verify_data.py`: Quick visual checks of conversion (saves verification plots).
- `scripts/03_prepare_dataset.py`: Creates `dataset/` structure and train/val splits.
- `scripts/04_train.py`: Trains YOLOv8 (reads `configs/experiment_config.yaml`).
- `scripts/05_evaluate.py`: Evaluates a weights file on validation set and prints metrics (use `--conf` to change threshold).
- `scripts/06_video_inference.py`: Runs inference on video files or VisDrone sequence folders, saves per-frame outputs and compiles MP4s.

Examples:

```bash
python scripts/05_evaluate.py --weights runs/detect/yolov8s_20260224_1745502/weights/best.pt --conf 0.001

python scripts/06_video_inference.py --video VisDrone2019-VID-val/sequences/uav0000137_00458_v --weights runs/detect/yolov8s_20260224_1745502/weights/best.pt --conf 0.3 --save
```

---

## Reproducibility

To reproduce the canonical evaluation exactly:

1. Ensure the environment matches `requirements.txt` (or install dependencies above).
2. Set `PYTHONPATH` to project root if needed:

```powershell
$env:PYTHONPATH = "$(Get-Location)"
```

3. Run evaluation:

```bash
python scripts/05_evaluate.py --weights runs/detect/yolov8s_20260224_1745502/weights/best.pt --conf 0.001
```

Optional: regenerate demo video for a chosen sequence:

```bash
python scripts/06_video_inference.py --video VisDrone2019-VID-val/sequences/uav0000137_00458_v --weights runs/detect/yolov8s_20260224_1745502/weights/best.pt --conf 0.3 --save
```

---

## References

- VisDrone 2019 Detection challenge: http://aiskyeye.com/
- Ultralytics YOLOv8: https://github.com/ultralytics/ultralytics

---
