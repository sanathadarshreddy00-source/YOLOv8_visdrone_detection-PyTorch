<<<<<<< HEAD
# YOLOv8 â€” VisDrone Detection (Canonical README)

One-line: Reproducible YOLOv8 detection pipeline for the VisDrone 2019 dataset â€” training, evaluation, and demo video generation.

---

## Quickstart

- Create and activate environment (example):

```powershell
conda create -n cv310 python=3.10 -y
conda activate cv310
```

- Install dependencies (if `requirements.txt` present):

```powershell
pip install -r requirements.txt
# or at minimum:
pip install ultralytics opencv-python pyyaml
```

- If Python cannot import `src`, set `PYTHONPATH` at project root before running scripts:

```powershell
$env:PYTHONPATH = "$(Get-Location)"
```

Run the three main steps (copy/paste):

```powershell
# Train (example)
=======
# VisDrone Object Detection with YOLOv8

Real-time object detection pipeline for aerial imagery using YOLOv8 on the VisDrone 2019 dataset.

**Project Status:** âœ… Complete  
**Achieved:** **36.2% mAP@0.5**
**Model:** YOLOv8s (11.1M parameters)

---

## ðŸŽ¯ Project Summary

Successfully trained and deployed YOLOv8s for detecting 10 object classes in dense aerial imagery. The project demonstrates end-to-end pipeline development from data conversion to video inference, with emphasis on handling extreme small-object detection challenges under hardware constraints.

---

## ðŸ“ Project Structure

```
Second Project/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ visdrone.yaml              # YOLOv8 dataset config
â”‚   â””â”€â”€ experiment_config.yaml     # Training hyperparameters
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ convert.py             # VisDrone â†’ YOLO conversion
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logging.py             # Experiment logging
â”‚       â”œâ”€â”€ reproducibility.py     # Seed management
â”‚       â”œâ”€â”€ visualization.py       # Plotting utilities
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_convert_annotations.py  # Convert VisDrone to YOLO format
â”‚   â”œâ”€â”€ 02_verify_data.py          # Visual verification
â”‚   â”œâ”€â”€ 03_prepare_dataset.py      # Create train/val split
â”‚   â”œâ”€â”€ 04_train.py                # Train YOLOv8
â”‚   â”œâ”€â”€ 05_evaluate.py             # Evaluate and compare
â”‚   â”œâ”€â”€ 06_video_inference.py      # Video sequence inference
â”œâ”€â”€ Annotations/_train/            # Original VisDrone annotations
â”œâ”€â”€ images1/images/                # Original images (1610 files)
â”œâ”€â”€ dataset/                       # Processed YOLO format dataset
â”‚   â”œâ”€â”€ images/train/              # 1288 training images
â”‚   â”œâ”€â”€ images/val/                # 322 validation images
â”‚   â”œâ”€â”€ labels/train/              # Training annotations
â”‚   â””â”€â”€ labels/val/                # Validation annotations
â”œâ”€â”€ runs/detect/                   # Training outputs
â”‚   â””â”€â”€ runs/videos/               # Demo videos
â”œâ”€â”€ VisDrone2019-VID-val/          # Video sequences for inference
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ðŸš€ Quick Start

### 1. Install Dependencies

```bash
# Activate your conda environment
conda activate cv310

# Install requirements
pip install -r requirements.txt
```

### 2. Run Pipeline

Execute scripts in order:

```bash
# Step 1: Convert annotations (VisDrone â†’ YOLO format)
python scripts/01_convert_annotations.py

# Step 2: Verify conversion visually
python scripts/02_verify_data.py

# Step 3: Prepare dataset (train/val split + file organization)
python scripts/03_prepare_dataset.py

# Step 4: Train YOLOv8 model
>>>>>>> origin/main
python scripts/04_train.py

# Evaluate canonical model (conf=0.001)
python scripts/05_evaluate.py --weights runs/detect/yolov8s_20260224_1745502/weights/best.pt --conf 0.001

# Generate demo video for a sequence
python scripts/06_video_inference.py --video VisDrone2019-VID-val/sequences/uav0000137_00458_v --weights runs/detect/yolov8s_20260224_1745502/weights/best.pt --conf 0.3 --save
```

---

## Canonical experiment (this README is written around this run)

- Experiment name: `yolov8s_20260224_1745502`
- Weights: `runs/detect/yolov8s_20260224_1745502/weights/best.pt`
- Training args (excerpt): `imgsz=640`, `batch=4`, `workers=0`, `seed=42`
- Eval command used for reported metrics:

```bash
python scripts/05_evaluate.py --weights runs/detect/yolov8s_20260224_1745502/weights/best.pt --conf 0.001
```

Canonical metrics (validation, conf=0.001):

- mAP@0.5: **36.35%**
- mAP@0.5:0.95: **21.21%**
- Precision: **47.10%**
- Recall: **36.35%**

Per-class mAP@0.5 (conf=0.001):

| Class | mAP@0.5 |
|---|---:|
| pedestrian | 21.76% |
| people | 13.75% |
| bicycle | 10.69% |
| car | 73.78% |
| van | 42.95% |
| truck | 56.66% |
| tricycle | 20.58% |
| awning-tricycle | 29.25% |
| bus | 66.97% |
| motor | 27.14% |

Demo examples (produced under `runs/detect/videos/`):

- Example demo folder: `runs/detect/videos/demo_20260224_224656/`
- Example MP4: `runs/detect/videos/demo_20260224_224656/uav0000137_00458_v.mp4` (233 frames processed)

---

## Project layout (key folders)

- `configs/` â€” dataset and experiment YAMLs (`visdrone.yaml`, `experiment_config.yaml`, `paths.yaml`).
- `src/` â€” reusable utilities (paths loader, reproducibility, visualization).
- `scripts/` â€” runnable scripts: conversion, verification, prepare dataset, train, evaluate, video inference.
- `tools/` â€” helper tools (inventory generation, maintenance).
- `runs/detect/` â€” training experiment folders and demo outputs (weights and demo videos).
- `VisDrone2019-VID-val/` â€” video sequences used for demo inference.

---

## Script usage (short)

- `scripts/01_convert_annotations.py`: Convert VisDrone annotations to YOLO format.
- `scripts/02_verify_data.py`: Quick visual checks of conversion (saves verification plots).
- `scripts/03_prepare_dataset.py`: Creates `dataset/` structure and train/val splits.
- `scripts/04_train.py`: Trains YOLOv8 (reads `configs/experiment_config.yaml`).
- `scripts/05_evaluate.py`: Evaluates a weights file on validation set and prints metrics (use `--conf` to change threshold).
- `scripts/06_video_inference.py`: Runs inference on video files or VisDrone sequence folders, saves per-frame outputs and compiles MP4s.

Examples:

```bash
python scripts/05_evaluate.py --weights runs/detect/yolov8s_20260224_1745502/weights/best.pt --conf 0.001

python scripts/06_video_inference.py --video VisDrone2019-VID-val/sequences/uav0000137_00458_v --weights runs/detect/yolov8s_20260224_1745502/weights/best.pt --conf 0.3 --save
```

---

## Artifacts & what NOT to commit

- Do NOT commit:
  - Large model files (`*.pt`, `*.pth`) â€” use Git LFS, DVC, S3, or GitHub Releases.
  - Full datasets and image folders (e.g., `images1/`, `VisDrone2019-*`) â€” provide download instructions instead.
  - Runtime outputs: `runs/`, `logs/`, `backups/`.

- Recommended: commit small manifests that point to artifacts (e.g., `configs/last_experiment_args.yaml`, `DIRECTORY_RECORD_SUMMARY.json`) and an `artifacts/weights_manifest.json` with checksums/URLs for large weights.

---

## Reproducibility

To reproduce the canonical evaluation exactly:

1. Ensure the environment matches `requirements.txt` (or install dependencies above).
2. Set `PYTHONPATH` to project root if needed:

```powershell
$env:PYTHONPATH = "$(Get-Location)"
```

3. Run evaluation:

```bash
python scripts/05_evaluate.py --weights runs/detect/yolov8s_20260224_1745502/weights/best.pt --conf 0.001
```

Optional: regenerate demo video for a chosen sequence:

```bash
python scripts/06_video_inference.py --video VisDrone2019-VID-val/sequences/uav0000137_00458_v --weights runs/detect/yolov8s_20260224_1745502/weights/best.pt --conf 0.3 --save
```

---

## References

- VisDrone 2019 Detection challenge: http://aiskyeye.com/
- Ultralytics YOLOv8: https://github.com/ultralytics/ultralytics

---